<!DOCTYPE html>
<html>
  <head>
    <title>VVT</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <link href="resources/css/axure_rp_page.css" type="text/css" rel="stylesheet"/>
    <link href="data/styles.css" type="text/css" rel="stylesheet"/>
    <link href="files/vvt/styles.css" type="text/css" rel="stylesheet"/>
    <script src="resources/scripts/jquery-3.2.1.min.js"></script>
    <script src="resources/scripts/axure/axQuery.js"></script>
    <script src="resources/scripts/axure/globals.js"></script>
    <script src="resources/scripts/axutils.js"></script>
    <script src="resources/scripts/axure/annotation.js"></script>
    <script src="resources/scripts/axure/axQuery.std.js"></script>
    <script src="resources/scripts/axure/doc.js"></script>
    <script src="resources/scripts/messagecenter.js"></script>
    <script src="resources/scripts/axure/events.js"></script>
    <script src="resources/scripts/axure/recording.js"></script>
    <script src="resources/scripts/axure/action.js"></script>
    <script src="resources/scripts/axure/expr.js"></script>
    <script src="resources/scripts/axure/geometry.js"></script>
    <script src="resources/scripts/axure/flyout.js"></script>
    <script src="resources/scripts/axure/model.js"></script>
    <script src="resources/scripts/axure/repeater.js"></script>
    <script src="resources/scripts/axure/sto.js"></script>
    <script src="resources/scripts/axure/utils.temp.js"></script>
    <script src="resources/scripts/axure/variables.js"></script>
    <script src="resources/scripts/axure/drag.js"></script>
    <script src="resources/scripts/axure/move.js"></script>
    <script src="resources/scripts/axure/visibility.js"></script>
    <script src="resources/scripts/axure/style.js"></script>
    <script src="resources/scripts/axure/adaptive.js"></script>
    <script src="resources/scripts/axure/tree.js"></script>
    <script src="resources/scripts/axure/init.temp.js"></script>
    <script src="resources/scripts/axure/legacy.js"></script>
    <script src="resources/scripts/axure/viewer.js"></script>
    <script src="resources/scripts/axure/math.js"></script>
    <script src="resources/scripts/axure/jquery.nicescroll.min.js"></script>
    <script src="data/document.js"></script>
    <script src="files/vvt/data.js"></script>
    <script type="text/javascript">
      $axure.utils.getTransparentGifPath = function() { return 'resources/images/transparent.gif'; };
      $axure.utils.getOtherPath = function() { return 'resources/Other.html'; };
      $axure.utils.getReloadPath = function() { return 'resources/reload.html'; };
    </script>
  </head>
  <body>
    <div id="base" class="">

      <!-- Unnamed (矩形) -->
      <div id="u0" class="ax_default box_1">
        <img id="u0_img" class="img " src="images/vvt/u0.svg"/>
        <div id="u0_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u1" class="ax_default box_1">
        <img id="u1_img" class="img " src="images/vvt/u1.svg"/>
        <div id="u1_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u2" class="ax_default box_2">
        <div id="u2_div" class=""></div>
        <div id="u2_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u3" class="ax_default _文本段落">
        <div id="u3_div" class=""></div>
        <div id="u3_text" class="text ">
          <p><span>Vision transformers have shown great success on numerous computer vision tasks. However, its central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images.</span></p><p><span>We investigate this problem and find that computer vision tasks focus more on local information compared with NLP tasks. Based on this observation, we present a Vicinity Attention that introduces a locality bias to vision transformers with linear complexity. Specifically, for each image patch, we enforce a cosine re-weighting mechanism based on&nbsp; the 2D Manhattan distance between its neighbouring patches. In this case, the neighbouring patches will receive stronger attention than far- 020 away patches. Moreover, since our Vicinity Attention requires the token length to be much larger than the feature dimension, we further propose a new Vicinity Vision Transformer (VVT), which reduces the feature dimension without degradation in performance. We perform extensive&nbsp; experiments on the CIFAR100, ImageNet1K, and ADE20K datasets to&nbsp; validate the effectiveness of our method. Our method has a slower growth rate of GFlops than previous transformer-based and convolution-based&nbsp; networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50% fewer parameters than previous methods.</span></p>
        </div>
      </div>

      <!-- paper文字 (矩形) -->
      <div id="u4" class="ax_default label" data-label="paper文字">
        <div id="u4_div" class=""></div>
        <div id="u4_text" class="text ">
          <p><span style="text-decoration:underline ;">Paper</span></p>
        </div>
      </div>

      <!-- 代码文字 (矩形) -->
      <div id="u5" class="ax_default label" data-label="代码文字">
        <div id="u5_div" class=""></div>
        <div id="u5_text" class="text ">
          <p><span style="text-decoration:underline ;">Code</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u6" class="ax_default _一级标题">
        <div id="u6_div" class=""></div>
        <div id="u6_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u7" class="ax_default _一级标题">
        <div id="u7_div" class=""></div>
        <div id="u7_text" class="text ">
          <p><span>Citation</span></p>
        </div>
      </div>

      <!-- Unnamed (热区) -->
      <div id="u8" class="ax_default">
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u9" class="ax_default _一级标题">
        <div id="u9_div" class=""></div>
        <div id="u9_text" class="text ">
          <p><span>Vicinity Vision Transformer</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u10" class="ax_default _二级标题">
        <div id="u10_div" class=""></div>
        <div id="u10_text" class="text ">
          <p><span>Abstract</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u11" class="ax_default _文本段落">
        <div id="u11_div" class=""></div>
        <div id="u11_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- 作者1 (矩形) -->
      <div id="u12" class="ax_default label" data-label="作者1">
        <div id="u12_div" class=""></div>
        <div id="u12_text" class="text ">
          <p><span style="text-decoration:underline ;">Zhen Qin</span></p>
        </div>
      </div>

      <!-- 作者2 (矩形) -->
      <div id="u13" class="ax_default label" data-label="作者2">
        <div id="u13_div" class=""></div>
        <div id="u13_text" class="text ">
          <p><span style="text-decoration:underline ;">Weixuan Sun</span></p>
        </div>
      </div>

      <!-- 作者3 (矩形) -->
      <div id="u14" class="ax_default label" data-label="作者3">
        <div id="u14_div" class=""></div>
        <div id="u14_text" class="text ">
          <p><span style="text-decoration:underline ;">Hui Deng</span></p>
        </div>
      </div>

      <!-- 作者4 (矩形) -->
      <div id="u15" class="ax_default label" data-label="作者4">
        <div id="u15_div" class=""></div>
        <div id="u15_text" class="text ">
          <p><span style="text-decoration:underline ;">Dongxu Li</span></p>
        </div>
      </div>

      <!-- 作者5 (矩形) -->
      <div id="u16" class="ax_default label" data-label="作者5">
        <div id="u16_div" class=""></div>
        <div id="u16_text" class="text ">
          <p><span style="text-decoration:underline ;">Yunshen Wei</span></p>
        </div>
      </div>

      <!-- 作者6 (矩形) -->
      <div id="u17" class="ax_default label" data-label="作者6">
        <div id="u17_div" class=""></div>
        <div id="u17_text" class="text ">
          <p><span style="text-decoration:underline ;">Baohong Lv</span></p>
        </div>
      </div>

      <!-- 作者7 (矩形) -->
      <div id="u18" class="ax_default label" data-label="作者7">
        <div id="u18_div" class=""></div>
        <div id="u18_text" class="text ">
          <p><span style="text-decoration:underline ;">Junjie Yan</span></p>
        </div>
      </div>

      <!-- 作者8 (矩形) -->
      <div id="u19" class="ax_default label" data-label="作者8">
        <div id="u19_div" class=""></div>
        <div id="u19_text" class="text ">
          <p><span style="text-decoration:underline ;">Lingpeng Kong</span></p>
        </div>
      </div>

      <!-- 作者9 (矩形) -->
      <div id="u20" class="ax_default label" data-label="作者9">
        <div id="u20_div" class=""></div>
        <div id="u20_text" class="text ">
          <p><span style="text-decoration:underline ;">Yiran Zhong</span></p>
        </div>
      </div>

      <!-- paper (图片 ) -->
      <div id="u21" class="ax_default image" data-label="paper">
        <img id="u21_img" class="img " src="images/vvt/paper_u21.png"/>
        <div id="u21_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- code (图片 ) -->
      <div id="u22" class="ax_default image" data-label="code">
        <img id="u22_img" class="img " src="images/vvt/code_u22.png"/>
        <div id="u22_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u23" class="ax_default _二级标题">
        <div id="u23_div" class=""></div>
        <div id="u23_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u24" class="ax_default _一级标题">
        <div id="u24_div" class=""></div>
        <div id="u24_text" class="text ">
          <p><span>Method</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u25" class="ax_default box_3">
        <div id="u25_div" class=""></div>
        <div id="u25_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u26" class="ax_default _文本段落">
        <div id="u26_div" class=""></div>
        <div id="u26_text" class="text ">
          <p><span>@misc{sun2022vicinity,</span></p><p><span>&nbsp;&nbsp; &nbsp;&nbsp; title={Vicinity Vision Transformer}, </span></p><p><span>&nbsp;&nbsp; &nbsp;&nbsp; author={Weixuan Sun and Zhen Qin and Hui Deng and Jianyuan Wang and Yi Zhang and Kaihao Zhang and Nick Barnes and Stan Birchfield and Lingpeng Kong and Yiran Zhong},</span></p><p><span>&nbsp;&nbsp; &nbsp;&nbsp; year={2022},</span></p><p><span>&nbsp;&nbsp; &nbsp;&nbsp; eprint={2206.10552},</span></p><p><span>&nbsp;&nbsp; &nbsp;&nbsp; archivePrefix={arXiv},</span></p><p><span>&nbsp;&nbsp; &nbsp;&nbsp; primaryClass={cs.CV}</span></p><p><span>}</span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u27" class="ax_default image">
        <img id="u27_img" class="img " src="images/vvt/u27.png"/>
        <div id="u27_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u28" class="ax_default label">
        <div id="u28_div" class=""></div>
        <div id="u28_text" class="text ">
          <p><span>Overall architecture of our VVT. VVT adopts a pyramid structure and is divided into four stages. The illustrated sample is VVT-Small. The output resolution is progressively shrinked to generate multi-scale feature maps.</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u29" class="ax_default _文本段落">
        <div id="u29_div" class=""></div>
        <div id="u29_text" class="text ">
          <p><span>To validate the effectiveness of our idea, we propose a general-purpose vision backbone called Vicinity Vision Transformer (VVT). VVT adopts a progressive shrinking pyramid structure and has four stages that generate feature maps at different scales. Each stage contains a patch embedding layer and multiple Vicinity Transformer Blocks. In detail, we use a patch size of</span></p><p><span>4×4 in the first stage. Given the input image of size H ×W ×3, we first divide it into H4 × W4 patches. Then we feed the patches into a patch embedding module to obtain a flattened embedding sequence with the size of HW × C . Here we adopt the patch embedding module proposed to project the patches into the target feature embedding. The embedding sequence is subsequently fed into several successive transformer blocks with L1 layers. Compared with the standard transformer block, we only replace the multi-head self-attention (MSA) module with our proposed linear Vicinity Attention Block, while other parts are kept the same. In the second stage, the feature sequence from the first stage are reshaped back to H4 × W4 × C1 and fed into the patch embedding module again. It is projected and down-sampled to a embedding sequence of size HW × C, and then processed by the transformer blocks of the second stage. We follow the same approach to obtain multi-scale output feature maps of the third and fourth stage with output sizes of H × W and H × W , respectively. Hierarchical feature maps can be easily leveraged to many downstream tasks such as im- age classification, semantic segmentation and object detection. Similar to other backbone methods, we introduce different architecture variants like VVT-Tiny or VVT-Large</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u30" class="ax_default box_3">
        <div id="u30_div" class=""></div>
        <div id="u30_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u31" class="ax_default _一级标题">
        <div id="u31_div" class=""></div>
        <div id="u31_text" class="text ">
          <p><span>Results</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u32" class="ax_default _文本段落">
        <div id="u32_div" class=""></div>
        <div id="u32_text" class="text ">
          <p><span>To verify the effectivenes of our method, we conduct extensive experiments on the CIFAR100 and ImageNet1K datasets for image classification, and on the ADE20K dataset for semantic segmentation. Specifically, we first make a comparison with existing state-of-the-art methods, and then give an ablation study over the design of VVT.</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u33" class="ax_default label">
        <div id="u33_div" class=""></div>
        <div id="u33_text" class="text ">
          <p style="text-align:center;"><span>Image Classification</span></p><p style="text-align:left;"><span><br></span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u34" class="ax_default image">
        <img id="u34_img" class="img " src="images/vvt/u34.png"/>
        <div id="u34_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u35" class="ax_default label">
        <div id="u35_div" class=""></div>
        <div id="u35_text" class="text ">
          <p><span>Comparison of different backbones on ImageNet1k[10]. All models are trained and tested on 224 × 224 resolution. GFLOPs is also calculated under the input scale of 224 × 224. MS out represents multi-scale output.</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u36" class="ax_default label">
        <div id="u36_div" class=""></div>
        <div id="u36_text" class="text ">
          <p><span>Semantic Segmentation</span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u37" class="ax_default image">
        <img id="u37_img" class="img " src="images/vvt/u37.png"/>
        <div id="u37_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u38" class="ax_default label">
        <div id="u38_div" class=""></div>
        <div id="u38_text" class="text ">
          <p><span>Semantic segmentation results on ADE20K validation set. Competing results come from PVTv2 and Twins. All backbone networks are pre-trained on ImageNet1k.</span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u39" class="ax_default image">
        <img id="u39_img" class="img " src="images/vvt/u39.png"/>
        <div id="u39_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u40" class="ax_default label">
        <div id="u40_div" class=""></div>
        <div id="u40_text" class="text ">
          <p><span>Qualitative results of semantic segmentation on ADE20K The semantic segmentation results are generated by VVT-L based Semantic FPN.</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u41" class="ax_default label">
        <div id="u41_div" class=""></div>
        <div id="u41_text" class="text ">
          <p><span>Ablation Study</span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u42" class="ax_default image">
        <img id="u42_img" class="img " src="images/vvt/u42.png"/>
        <div id="u42_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u43" class="ax_default label">
        <div id="u43_div" class=""></div>
        <div id="u43_text" class="text ">
          <p><span>Analysis of locality constraint on Cifar100 and ImageNet1K. The VVT-softmax represents the variant that replaces all Vicinity Attention with the standard softmax attention.</span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u44" class="ax_default image">
        <img id="u44_img" class="img " src="images/vvt/u44.png"/>
        <div id="u44_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u45" class="ax_default label">
        <div id="u45_div" class=""></div>
        <div id="u45_text" class="text ">
          <p><span>We compare the class-wise attention maps of the VVT against the VVT w/o locality, the PVT and the ViT. The attention maps are obtained using Grad CAM. By enforcing the locality bias, the activation areas are more concentrated and correctly locate on the object.</span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u46" class="ax_default image">
        <img id="u46_img" class="img " src="images/vvt/u46.png"/>
        <div id="u46_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u47" class="ax_default label">
        <div id="u47_div" class=""></div>
        <div id="u47_text" class="text ">
          <p><span>GFLOPs of VVT using different feature reduction rates. When FR ratio is increased larger than 4, the growth ratio tends to be saturated. </span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u48" class="ax_default image">
        <img id="u48_img" class="img " src="images/vvt/u48.png"/>
        <div id="u48_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u49" class="ax_default label">
        <div id="u49_div" class=""></div>
        <div id="u49_text" class="text ">
          <p><span>Results of VVT using different feature reduction ratio(FR). We empirically find that the FR=2 leads to the best performance.</span></p>
        </div>
      </div>

      <!-- Unnamed (图片 ) -->
      <div id="u50" class="ax_default image">
        <img id="u50_img" class="img " src="images/vvt/u50.png"/>
        <div id="u50_text" class="text " style="display:none; visibility: hidden">
          <p></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u51" class="ax_default label">
        <div id="u51_div" class=""></div>
        <div id="u51_text" class="text ">
          <p><span>Top-1 accuracy of our Vicinity Vision Transformer (VVT) with respect to parameters on the ImageNet-1k [18] dataset, and the GFLOPs</span></p><p><span>corresponding to various input image sizes. Our VVT outperforms all competitors with 50% fewer parameters, and it enjoys the lowest GFLOPs growth rate.</span></p>
        </div>
      </div>

      <!-- Unnamed (矩形) -->
      <div id="u52" class="ax_default label">
        <div id="u52_div" class=""></div>
        <div id="u52_text" class="text ">
          <p><span>Performances Overview</span></p>
        </div>
      </div>
    </div>
    <script src="resources/scripts/axure/ios.js"></script>
  </body>
</html>
